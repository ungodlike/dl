
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 5000)

# Pad sequences to the same length
x_train = sequence.pad_sequences(x_train, maxlen=500)
x_test = sequence.pad_sequences(x_test, maxlen=500)

#Creating Model
model = Sequential()
model.add(Embedding(5000, 128))
model.add(LSTM(128,activation="tanh",recurrent_activation="sigmoid"))
model.add(Dense(1, activation = 'sigmoid'))
model.summary()

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#training
lstm=model.fit(x_train, y_train, batch_size= 32, epochs= 10,validation_data=(x_test, y_test),shuffle=True,verbose=1)

op=model.predict(x_test)

from random import randint
arr_ind=randint(0,24999)
index=imdb.get_word_index()
reverse_index = dict([(value, key) for (key, value) in index. items()])
decoded = "".join([reverse_index.get(i - 3, "#") for i in x_test[arr_ind]])
arr=[]
for i in op:
    if(i<0.5):
        arr .append ("Negative")
    else:
        arr. append ("Positive")
print("Sentence: ", decoded)
print("Review:", arr[arr_ind])
print("Predicted Value" ,op[arr_ind][0])
print("Expected Value",y_test[arr_ind])
